geom_point(shape = 3) + geom_line(linewidth = 1, color = "dodgerblue1")
ggplot(data = data, aes(x = nclusters)) +
geom_point(shape = 3) + geom_line(linewidth = 1, color = "dodgerblue1")
# Elección de número de clusters
linkages = c("single", "complete", "average", "centroid", "ward.D", "ward.D2")
#png(filename = "AllCalinski.png", width = 600, res = 120)
par(mfrow=c(2,3))
for (linkage in linkages) {
fviz_hierarchical_ch(x, linkage)
}
mtext("Calinksi-Harabasz index", outer = T, side = 3, line = -2)
fviz_hierarchical_ch <- function(data, linkage) {
ch <- c()
#linkages <- c("single", "complete", "average", "ward.D", "ward.D2", "centroid", "mcquitty")
distanceMatrix <- dist(data, method = "binary")
for (i in 2:6) {
#km <- kmeans(data, i) # perform clustering
hcClustering <- cutree(hclust(distanceMatrix, method = linkage), k = i)
ch[i] <- calinhara(data, # data
hcClustering, # cluster assignments
cn=max(hcClustering) # total cluster number
)
}
ch <-ch[2:6]
k <- 2:6
plot(k, ch,
lty = 1, type = "o", lwd = 1, pch = 4,
ylab = "",
xlab = "Number of clusters",
main = paste(linkage, "linkage"), cex.main = 0.8,
col = "dodgerblue1", cex = 0.9 ,
bty = "l",
las= 1, cex.axis = 0.8, tcl = -0.2)
#plot(k, ch,xlab =  "Cluster number k",
#     ylab = "Caliński - Harabasz Score",
#     main = paste("Caliński - Harabasz Plot, linkage:", linkage), cex.main=1,
#     col = "dodgerblue1", cex = 0.9 ,
#     lty=1 , type="o" , lwd=1, pch=4,
#     bty = "l",
#     las = 1, cex.axis = 0.8, tcl  = -0.2)
abline(v=which(ch==max(ch)) + 1, lwd=1, col="red", lty="dashed")
print(max(ch))
return(ch)
}
#png(filename = "AllCalinski.png", width = 600, res = 120)
par(mfrow=c(2,3))
for (linkage in linkages) {
fviz_hierarchical_ch(x, linkage)
}
# Nos quedamos con 3 clusters, y con el de Ward D2, que parece haber logrado
#un mayor Calinksi
distanceMatrix <- dist(x, method = "binary")
clusters <- 3
clus.boot <- clusterboot(distanceMatrix,
distances = TRUE,
B=1000, # Number of bootstrap resamples
clustermethod=hclustCBI, # for hierarchical clustering
method="ward.D2", # use what we used in "hclust"
k=clusters,
count=F) # Show progress on screen?
# clus.boot
AvgJaccard <- clus.boot$bootmean
clus.boot
# clus.boot
AvgJaccard <- clus.boot$bootmean
Instability <- clus.boot$bootbrd/1000
Clusters <- c(1:clusters)
Eval <- cbind(Clusters, AvgJaccard, Instability)
Eval
hcWard2 <- hclust(distanceMatrix, method = "ward.D2")
dendrogram(hcWard2)
plot(hcWard2)
#Very stable!
par(mfrow = c(1,1))
plot(hcWard2)
plot(hcWard2, labels = F)
rect.hclust(hcWard2, k=3)
cutree(hcWard, k = 3)
cutree(hcWard2, k = 3)
#Clusterplot
groups <- cutree(hcWard2, k = clusters)
groups <- as.factor(groups)
clusplot(x, groups, color = T, shade = T, labels = clusters, lines = 0)
clusplot(x, groups, color = T, shade = T, labels = 4, lines = 0)
princomp(x)
biplot(princomp(x))
biplot(princomp(x), labels = F)
PCA <- function(X, warn = TRUE)
{
ndf <- nrow(X) - 1
nvar <- ncol(X)
X.svd <- svd(X, nu = min(ndf,nvar), nv = min(ndf,nvar))
X.svd$d <- X.svd$d[1:min(ndf,nvar)]
varnames <- colnames(X)
if (is.null(varnames)) varnames <- paste("Var", 1:ncol(X))
object <- list(scores = X.svd$u %*% diag(X.svd$d),
loadings = X.svd$v,
var = X.svd$d^2 / ndf,
totalvar = sum(X.svd$d^2)/ndf)
dimnames(object$scores) <- list(rownames(X),
paste("PC", 1:ncol(object$scores)))
dimnames(object$loadings) <- list(varnames,
paste("PC", 1:ncol(object$loadings)))
names(object$var) <- paste("PC", 1:length(X.svd$d))
if (!isTRUE(all.equal(colMeans(X), rep(0, ncol(X)),
check.attributes = FALSE))) {
if (warn)
warning("Performing PCA on a non-meancentered data matrix!")
object$centered.data <- FALSE
} else {
object$centered.data <- TRUE
}
class(object) <- "PCA"
object
}
PCA(x)
pca <- PCA(x)
pca$scores
scores
pca$scores
pca$scores$PC1
pca$scores[:,1]
pca$scores[,1]
ggplot(data = pca, aes(x= scores[,1], y = scores[,2])) +
geom_point()
ggplot(data = pca$scores, aes(x= scores[,1], y = scores[,2])) +
geom_point()
pcaScores <- dataframe(pca$scores)
pcaScores <- data.frame(pca$scores)
ggplot(data = pcaScores, aes(x= scores[,1], y = scores[,2])) +
geom_point()
pcaScores
ggplot(data = pcaScores, aes(x= PC.1, y = PC.2)) +
geom_point()
pca <- PCA(distanceMatrix)
pcaScores <- data.frame(pca$scores)
ggplot(data = pcaScores, aes(x= PC.1, y = PC.2)) +
geom_point()
fviz_hierarchical_ch <- function(data, linkage) {
ch <- c()
#linkages <- c("single", "complete", "average", "ward.D", "ward.D2", "centroid", "mcquitty")
distanceMatrix <- dist(data, method = "binary")
for (i in 2:6) {
#km <- kmeans(data, i) # perform clustering
hcClustering <- cutree(hclust(distanceMatrix, method = linkage), k = i)
ch[i] <- calinhara(data, # data
hcClustering, # cluster assignments
cn=max(hcClustering) # total cluster number
)
}
ch <-ch[2:6]
k <- 2:6
plot(k, ch,
lty = 1, type = "o", lwd = 1, pch = 4,
ylab = "",
xlab = "Number of clusters",
main = paste(linkage, "linkage"), cex.main = 0.8,
col = "dodgerblue1", cex = 0.9 ,
bty = "l",
las= 1, cex.axis = 0.8, tcl = -0.2)
#plot(k, ch,xlab =  "Cluster number k",
#     ylab = "Caliński - Harabasz Score",
#     main = paste("Caliński - Harabasz Plot, linkage:", linkage), cex.main=1,
#     col = "dodgerblue1", cex = 0.9 ,
#     lty=1 , type="o" , lwd=1, pch=4,
#     bty = "l",
#     las = 1, cex.axis = 0.8, tcl  = -0.2)
abline(v=which(ch==max(ch)) + 1, lwd=1, col="red", lty="dashed")
print(max(ch))
return(ch)
}
pca <- PCA(x)
pcaScores <- data.frame(pca$scores)
ggplot(data = pcaScores, aes(x= PC.1, y = PC.2)) +
geom_point()
pca$var
pca$var[1,2]
pca$var[1:2]
pca$var[1] + pca$var[2]
pca$var[1] + pca$var[2]
(pca$var[1] + pca$var[2])/pca$totalvar
ggplot(data = pcaScores, aes(x= PC.1, y = PC.2, col = groups)) +
geom_point()
ggplot(data = pcaScores, aes(x= PC.1, y = PC.2, col = groups)) +
geom_point(size = 3)
data <- read.csv("data.csv")
x <- data[, -c(1:9)]
distanceMatrix <- dist(x, method = "binary")
# K-medoids
pam(distanceMatrix, 3, metric = "euclidean")
library(factoextra)
# K-medoids
pam(distanceMatrix, 3)
class()
class(distanceMatrix)
print(pam.res)
# K-medoids
pam.res <- pam(distanceMatrix, 3)
print(pam.res)
pam.res$medoids
pam.res$clustering
fviz_cluster(pam.res,
palette =c("#007892","#D9455F"),
ellipse.type ="euclid",
repel =TRUE,
ggtheme =theme_minimal())
fviz_cluster(pam.res,
palette =c("#007892","#D9455F", "yellow"),
ellipse.type ="euclid",
repel =TRUE,
ggtheme =theme_minimal())
fviz_cluster(pam.res, data = x,
palette =c("#007892","#D9455F", "yellow"),
ellipse.type ="euclid",
repel =TRUE,
ggtheme =theme_minimal())
fviz_cluster(pam.res, data = x,
palette =c("#007892","#D9455F", "yellow"),
ellipse.type ="euclid",
repel =TRUE,
ggtheme =theme_minimal())
fviz_cluster(pam.res, data = distanceMatrix,
palette =c("#007892","#D9455F", "yellow"),
ellipse.type ="euclid",
repel =TRUE,
ggtheme =theme_minimal())
fviz_cluster(pam.res, x,
palette =c("#007892","#D9455F", "yellow"),
ellipse.type ="euclid",
repel =TRUE,
ggtheme =theme_minimal())
fviz_cluster(pam.res, as.vector(x),
palette =c("#007892","#D9455F", "yellow"),
ellipse.type ="euclid",
repel =TRUE,
ggtheme =theme_minimal())
fviz_cluster(pam.res, data = as.vector(x),
palette =c("#007892","#D9455F", "yellow"),
ellipse.type ="euclid",
repel =TRUE,
ggtheme =theme_minimal())
fviz_cluster(pam.res)
fviz_cluster(pam.res, data = x)
fviz_cluster(pam.res, data = data.frame(x))
fviz_cluster(pam.res$clustering, data = as.vector(x),
palette =c("#007892","#D9455F", "yellow"),
ellipse.type ="euclid",
repel =TRUE,
ggtheme =theme_minimal())
fviz_cluster(pam.res, geom = "point", ellipse.type = "norm")
library(cluster)
fviz_cluster(pam.res, geom = "point", ellipse.type = "norm")
library(fpc)
library(cluster)
library(mclust)
library(Kmedians)
library(factoextra)
fviz_cluster(pam.res, geom = "point", ellipse.type = "norm")
fviz_cluster(pam.res, data = as.vector(x),
palette =c("#007892","#D9455F", "yellow"),
ellipse.type ="euclid",
repel =TRUE,
ggtheme =theme_minimal())
data("iris")
iris.scaled <- scale(iris[,-5])
pam.res <- pam(iris.scaled, 3)
fviz_cluster(pam.res, geom = "point", ellipse.type = "norm")
x.scaled <- scale(x)
distanceMatrix <- dist(x, method = "binary")
# K-medoids
pam.res <- pam(distanceMatrix, 3)
print(pam.res)
fviz_cluster(pam.res, x,
palette =c("#007892","#D9455F", "yellow"),
ellipse.type ="euclid",
repel =TRUE,
ggtheme =theme_minimal())
c(length(x), 1L)
names(x)
class(x)
class(iris.scaled)
matrix(data = x)
x
as.matrix(x)
xMatrix <- as.matrix(x)
class(xMatrix)
fviz_cluster(pam.res, xMatrix,
palette =c("#007892","#D9455F", "yellow"),
ellipse.type ="euclid",
repel =TRUE,
ggtheme =theme_minimal())
iris.scaled
attr(iris.scaled)
iris
require(cluster)
fviz_cluster(pam.res, x,
palette =c("#007892","#D9455F", "yellow"),
ellipse.type ="euclid",
repel =TRUE,
ggtheme =theme_minimal())
# Hierarchical clustering
# ++++++++++++++++++++++++
# Use hcut() which compute hclust and cut the tree
hc.cut <- hcut(distanceMatrix, k = 3, hc_method = "ward.D2")
plot(hc.cut)
# Visualize dendrogram
fviz_dend(hc.cut, show_labels = FALSE, rect = TRUE)
# Visualize cluster
fviz_cluster(hc.cut, ellipse.type = "convex")
# Visualize cluster
fviz_cluster(hc.cut, x,  ellipse.type = "convex")
# Visualize cluster
fviz_cluster(hc.cut, x,  ellipse.type = "convex", show_labels = F)
fviz_cluster(pam.res, x,
palette =c("#007892","#D9455F", "yellow"),
ellipse.type ="convex",
repel =TRUE,
ggtheme =theme_minimal())
# Visualize cluster
fviz_cluster(hc.cut, x,  ellipse.type = "euclid", show_labels = F)
# Visualize cluster
fviz_cluster(hc.cut, x,  ellipse.type = "ellipse", show_labels = F)
# Visualize cluster
fviz_cluster(hc.cut, x,  ellipse.type = "euclid", show_labels = F)
# Visualize cluster
fviz_cluster(hc.cut, x,  ellipse.type = "norm", show_labels = F)
x <- rawData[-c(1:9)]
# Hierarchical clustering
# ++++++++++++++++++++++++
# Use hcut() which compute hclust and cut the tree
hc.cut <- hcut(distanceMatrix, k = 3, hc_method = "ward.D2")
# Nos quedamos con 3 clusters, y con el de Ward D2, que parece haber logrado
#un mayor Calinksi
distanceMatrix <- dist(x, method = "binary")
# Hierarchical clustering
# ++++++++++++++++++++++++
# Use hcut() which compute hclust and cut the tree
hc.cut <- hcut(distanceMatrix, k = 3, hc_method = "ward.D2")
# Visualize dendrogram
fviz_dend(hc.cut, show_labels = FALSE, rect = TRUE)
# Visualize cluster
fviz_cluster(hc.cut, x,  ellipse.type = "norm", show_labels = F)
# Visualize cluster
fviz_cluster(hc.cut, x, show_labels = F)
# Visualize cluster
fviz_cluster(hc.cut, x, ellipse.type = "confidence" show_labels = F)
# Visualize cluster
fviz_cluster(hc.cut, x, ellipse.type = "confidence", show_labels = F)
# Visualize cluster
fviz_cluster(hc.cut, x, show_labels = F)
fviz_cluster(pam.res,
palette =c("#007892","#D9455F", "yellow"),
ellipse.type ="convex",
repel =TRUE,
ggtheme =theme_minimal())
fviz_cluster(pam.res)
data <- read.csv("data.csv")
x <- data[, -c(1:9)]
distanceMatrix <- dist(x, method = "binary")
# K-medoids
pam.res <- pam(distanceMatrix, 3)
print(pam.res)
fviz_cluster(pam.res)
fviz_cluster(pam.res, data = x)
fviz_cluster(pam.res, data = T)
hc.cut
pam.res
pam.res$clustering
pam.res$diss
pam.res$clusinfo
ggplot(data = pcaScores, aes(x= PC.1, y = PC.2, col = groups)) +
geom_point(size = 3)
ggplot(data = pcaScores, aes(x= PC.1, y = PC.2, col = pam.res$clustering)) +
geom_point(size = 3)
pam.res$clustering <- as.factor(pam.res$clustering)
pca <- PCA(x)
pcaScores <- data.frame(pca$scores)
ggplot(data = pcaScores, aes(x= PC.1, y = PC.2, col = pam.res$clustering)) +
geom_point(size = 3)
fviz_cluster(pam.res, x)
fviz_nbclust(x, diss = distanceMatrix, pam)
fviz_nbclust(x, diss = distanceMatrix, hcut(hc_method = "ward.D2"))
fviz_nbclust(x, diss = distanceMatrix, hcut, method = "ward.D2")
fviz_nbclust(x, diss = distanceMatrix, method = "silhouette" hcut, method = "ward.D2")
fviz_nbclust(x, diss = distanceMatrix, method = "silhouette", hcut, method = "ward.D2")
fviz_nbclust(x,hcut, diss = distanceMatrix, method = "silhouette",method = "ward.D2")
fviz_nbclust(x, FUN = hcut, diss = distanceMatrix, method = "silhouette",..., method = "ward.D2")
fviz_nbclust(x, FUN = hcut, diss = distanceMatrix, method = "silhouette", method = "ward.D2")
fviz_nbclust(x, FUN = hcut, diss = distanceMatrix, method = "silhouette",...)
fviz_nbclust(x, FUN = hcut(method = "ward.D2"), diss = distanceMatrix, method = "silhouette",...)
fviz_nbclust(x, FUN = hcut, diss = distanceMatrix, method = "silhouette",..., method = "ward.D2")
fviz_nbclust(x, hcut, diss = distanceMatrix, method = "silhouette", hc_method = "ward.D2")
fviz_nbclust(x, hcut, diss = distanceMatrix, method = "gap", hc_method = "ward.D2")
fviz_nbclust(x, hcut, diss = distanceMatrix, k.max = 6, method = "wss", hc_method = "ward.D2")
fviz_nbclust(x, hcut, diss = distanceMatrix, k.max = 6, method = "silhouette", hc_method = "ward.D2")
#Comprobamos con Silhouette
fviz_nbclust(x, hcut, diss = distanceMatrix, k.max = 6, method = "silhouette", hc_method = "single")
#Comprobamos con Silhouette
fviz_nbclust(x, hcut, diss = distanceMatrix, k.max = 6, method = "silhouette", hc_method = "complete")
#Comprobamos con Silhouette
fviz_nbclust(x, hcut, diss = distanceMatrix, k.max = 6, method = "silhouette", hc_method = "ward")
#Comprobamos con Silhouette
fviz_nbclust(x, hcut, diss = distanceMatrix, k.max = 6, method = "silhouette", hc_method = "ward.D")
#Comprobamos con Silhouette
fviz_nbclust(x, hcut, diss = distanceMatrix, k.max = 6, method = "silhouette", hc_method = "average")
#Comprobamos con Silhouette
fviz_nbclust(x, hcut, diss = distanceMatrix, k.max = 6, method = "silhouette", hc_method = "ward.D2")
fviz_partitional_ch <- function(data) {
ch <- c()
distanceMatrix <- dist(data, method = "binary")
for (i in 2:6) {
pam.res <- pam(distanceMatrix, i)
ch[i] <- calinhara(data, # data
pam.res$clustering, # cluster assignments
cn=max(pam.res$clustering) # total cluster number
)
}
ch <-ch[2:6]
k <- 2:6
plot(k, ch,
lty = 1, type = "o", lwd = 1, pch = 4,
ylab = "",
xlab = "Number of clusters",
main = paste(linkage, "linkage"), cex.main = 0.8,
col = "dodgerblue1", cex = 0.9 ,
bty = "l",
las= 1, cex.axis = 0.8, tcl = -0.2)
abline(v=which(ch==max(ch)) + 1, lwd=1, col="red", lty="dashed")
print(max(ch))
}
fviz_partitional_ch(x)
plot(k, ch,
lty = 1, type = "o", lwd = 1, pch = 4,
ylab = "",
xlab = "Number of clusters",
main = "K-medoids", cex.main = 0.8,
col = "dodgerblue1", cex = 0.9 ,
bty = "l",
las= 1, cex.axis = 0.8, tcl = -0.2)
fviz_partitional_ch <- function(data) {
ch <- c()
distanceMatrix <- dist(data, method = "binary")
for (i in 2:6) {
pam.res <- pam(distanceMatrix, i)
ch[i] <- calinhara(data, # data
pam.res$clustering, # cluster assignments
cn=max(pam.res$clustering) # total cluster number
)
}
ch <-ch[2:6]
k <- 2:6
plot(k, ch,
lty = 1, type = "o", lwd = 1, pch = 4,
ylab = "",
xlab = "Number of clusters",
main = "K-medoids", cex.main = 0.8,
col = "dodgerblue1", cex = 0.9 ,
bty = "l",
las= 1, cex.axis = 0.8, tcl = -0.2)
abline(v=which(ch==max(ch)) + 1, lwd=1, col="red", lty="dashed")
print(max(ch))
}
fviz_partitional_ch(x)
# De acuerdo con Calinski y Silhouette, tomamos 3 clusters
clusters <- 3
pam.res <- pam(distanceMatrix, clusters)
print(pam.res)
pam.res$clustering <- as.factor(pam.res$clustering)
pca <- PCA(x)
pcaScores <- data.frame(pca$scores)
ggplot(data = pcaScores, aes(x= PC.1, y = PC.2, col = pam.res$clustering)) +
geom_point(size = 3)
# Evaluación de su estabilidad
clus.boot <- clusterboot(distanceMatrix,
distances = TRUE,
B=1000, # Number of bootstrap resamples
clustermethod=claraCBI, # for hierarchical clustering
k=clusters,
count=T) # Show progress on screen?
# clus.boot
AvgJaccard <- clus.boot$bootmean
Instability <- clus.boot$bootbrd/1000
Clusters <- c(1:clusters)
Eval <- cbind(Clusters, AvgJaccard, Instability)
Eval
library(ClusterR)
gmm <- GMM(x, 3, dist_mode = "maha_dist", seed_mode = "random_subset")
library(ClusterR)
gmm <- GMM(x, 3, dist_mode = "maha_dist", seed_mode = "random_subset")
gmm
opt_gmm <- Optimal_Clusters_GMM(x, max_clusters = 6, criterion = "BIC",
dist_mode = "maha_dist", seed_mode = "random_subset",
km_iter = 10, em_iter = 10, var_floor = 1e-10,
plot_data = T)
opt_gmm <- Optimal_Clusters_GMM(x, max_clusters = 6, criterion = "AIC",
dist_mode = "maha_dist", seed_mode = "random_subset",
km_iter = 10, em_iter = 10, var_floor = 1e-10,
plot_data = T)
opt_gmm <- Optimal_Clusters_GMM(x, max_clusters = 12, criterion = "AIC",
dist_mode = "maha_dist", seed_mode = "random_subset",
km_iter = 10, em_iter = 10, var_floor = 1e-10,
plot_data = T)
opt_gmm <- Optimal_Clusters_GMM(x, max_clusters = 30, criterion = "AIC",
dist_mode = "maha_dist", seed_mode = "random_subset",
km_iter = 10, em_iter = 10, var_floor = 1e-10,
plot_data = T)
library(mltools)
one_hot(x)
library(data.table)
newx <- one_hot(as.data.table(x))
newx
newx$EFF00001
